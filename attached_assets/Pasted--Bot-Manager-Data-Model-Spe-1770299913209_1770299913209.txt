좋습니다. 그럼 약속한 대로 **구조부터 고정**합니다.
아래 문서는 이 프로젝트의 **기초 공사 도면**입니다.

---

# Bot Manager – Data Model Spec v0.1

이 문서는:

* “무엇을 저장하는 시스템인가”
* “데이터가 어떻게 분리되고 연결되는가”
* “어디서 섞이면 안 되는가”
  를 **코드보다 먼저** 고정하기 위한 기준 문서입니다.

---

## 1. 핵심 설계 원칙

1. 모든 데이터는 **Topic**으로 분리된다
2. 모든 봇 실행은 **Profile** 단위로 동작한다
3. Preset / Variant는 **동작 유형을 규정하는 메타데이터**다
4. Collect → Analyze → Output 파이프라인은 **Profile + Topic 기준으로만** 흐른다
5. 서로 다른 Topic의 데이터는 **절대 섞이지 않는다**

---

## 2. 주요 엔티티(테이블) 목록

필수 테이블:

* users
* profiles
* presets
* sources
* items
* analysis
* outputs (reports / drafts / alerts 통합 테이블 또는 분리 가능)
* jobs (스케줄 실행 관리, 선택)

---

## 3. 테이블별 역할과 필드 (개념 스키마)

### 3.1 users

사용자 계정

* id (PK)
* email
* name
* created_at

---

### 3.2 presets

봇의 “종류 템플릿”

예:

* daily_market_brief (report)
* community_engagement_helper (draft)
* competitor_watch (report/alert)

필드:

* id (PK)
* key (예: daily_market_brief)
* name
* description
* output_type (report | draft | alert)
* created_at

---

### 3.3 profiles  ⭐ 핵심

사용자가 만든 “내 봇 설정”

* id (PK)
* user_id (FK → users.id)
* preset_id (FK → presets.id)
* variant_key (예: ai_art, tech, startup, faith, investing 등)
* topic (예: investing, ai_art, tech 등)
* name (사용자 지정 이름)
* schedule_cron (또는 schedule_type + schedule_value)
* timezone
* config_json
  (중요도 가중치, 섹션 on/off, 길이, 기타 옵션)
* is_active (boolean)
* created_at

의미:

* **모든 실행은 profile 단위**
* 같은 Preset이라도 Profile이 다르면 완전히 다른 봇

---

### 3.4 sources

콘텐츠 입력 소스

* id (PK)
* name
* type (rss, feed, api, etc)
* url
* topic
* is_default (시스템 추천 소스인지)
* created_at

주의:

* Source는 반드시 **topic**을 가진다
* Profile은 여러 Source를 선택해서 쓴다

추가 연결 테이블:

* profile_sources

  * profile_id
  * source_id

---

### 3.5 items

수집된 원본 콘텐츠

* id (PK)
* source_id (FK → sources.id)
* topic
* external_id (중복 방지용)
* url
* title
* content_text
* published_at
* collected_at
* status (new | analyzed | processed)

중요 규칙:

* item.topic은 source.topic과 **반드시 일치**
* 다른 topic으로 이동 금지

---

### 3.6 analysis

분석 결과

* id (PK)
* item_id (FK → items.id)
* topic
* relevance_score
* importance_score
* risk_score
* category
* summary_short
* summary_long
* flags_json
* created_at

중요:

* analysis.topic = item.topic
* 이 단계에서 **중복 제거, 중요도 판단** 수행

---

### 3.7 outputs

최종 산출물 (리포트 / 초안 / 알림)

통합 테이블 방식:

* id (PK)
* profile_id (FK → profiles.id)
* preset_id (FK → presets.id)
* topic
* output_type (report | draft | alert)
* title
* content_text
* period_start
* period_end
* created_at

또는 분리:

* reports
* drafts
* alerts

중요 규칙:

* output.topic = profile.topic
* output은 **반드시 profile 단위**로 생성됨

---

### 3.8 jobs (선택, 하지만 권장)

스케줄 실행 관리

* id (PK)
* profile_id
* job_type (collect | analyze | output)
* last_run_at
* next_run_at
* status

---

## 4. 파이프라인 데이터 흐름

### Collect Job

* 입력: sources
* 출력: items
* 규칙: source.topic → item.topic 그대로 복사

### Analyze Job

* 입력: items where status = new
* 출력: analysis
* 규칙: item.topic 그대로 유지
* item.status = analyzed

### Output Job (Report / Draft / Alert)

* 입력:

  * profile
  * analysis where analysis.topic = profile.topic
* 출력:

  * outputs (profile_id 기준)
* 규칙:

  * **다른 topic 데이터 접근 금지**
  * profile 설정에 따라 필터링/집계

---

## 5. “섞임 방지” 강제 규칙

1. items.topic ≠ sources.topic → 저장 불가
2. analysis.topic ≠ items.topic → 저장 불가
3. outputs.topic ≠ profiles.topic → 생성 불가
4. Output Job은 반드시:

   * profile_id 기준
   * topic 기준
   * time window 기준
     으로만 데이터 조회

---

## 6. 이 v0.1에서 의도적으로 뺀 것

* 말투/스타일
* 자동 게시
* 크롤러
* 외부 연동(Slack, Email 등)
* 팀/조직 계정

이것들은 **Data Model v0.2 이후**에 추가

---

## 7. 이 문서의 위치

이 문서는:

> “이 시스템이 어떤 데이터 구조를 가져야 하는가”를 정의하는 **최상위 기준 문서**

앞으로:

* API 설계
* Job 설계
* UI 설계
* 프리셋 추가
* 기능 추가

모두 **이 구조 위에서만** 이루어져야 합니다.

---

## 8. 다음 단계 (권장 순서)

다음 문서:

> **Bot Manager – Pipeline & Job Spec v0.1**

* Collect / Analyze / Output Job이
* 어떤 조건으로
* 어떤 테이블을 읽고/쓰는지
* 어떤 스케줄로 도는지
  를 정확히 정의

---

지금 이 문서가 “기초 공사 도면”입니다.
여기서 마음에 안 드는 구조나, 바꾸고 싶은 개념(예: Preset/Variant/Topic/Profile 역할)이 있으면 **지금** 고치는 게 가장 쌉니다.

다음 단계로 **Pipeline & Job Spec v0.1**로 넘어갈까요?
